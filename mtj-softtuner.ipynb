{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryvLZsoZoE02"
   },
   "source": [
    "# [mtj-softtuner](https://github.com/VE-FORBRYDERNE/mtj-softtuner) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/VE-FORBRYDERNE/mtj-softtuner/blob/main/mtj-softtuner.ipynb) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
    "### (Unofficial Mesh Transformer JAX soft-tuning notebook)\n",
    "\n",
    "Create, in Colab, soft prompts compatible with KoboldAI (United) and [mkultra](https://github.com/corolla-johnson/mkultra) for your favourite GPT-J-6B-based or GPT-Neo-2.7B-based model!\n",
    "\n",
    "See this paper https://arxiv.org/pdf/2104.08691.pdf for more information about what a soft prompt is.\n",
    "\n",
    "---\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0). Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxhO67BVSH4O"
   },
   "source": [
    "# 1. Install and set up dependencies\n",
    "\n",
    "## If you, at any point, restart your Colab instance by using Runtime > Restart Runtime, you will have to run this cell again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBUVZ82oUmsk"
   },
   "source": [
    "Run the cell below to do some basic setup. Sometimes this cell may throw an error about \"deadline exceeded\", in which case you should restart your Colab instance (Runtime > Restart Runtime) and then run this cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-fuUNFL8JlET"
   },
   "outputs": [],
   "source": [
    "# @markdown There is an extremely large amount of code in this cell, so the code is hidden by default. Feel free to press \"Show code\" and look through the code, though.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import termcolor\n",
    "\n",
    "\n",
    "class NotebookException(Exception):\n",
    "    \"\"\"This kind of exception will not show a traceback when raised\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "ipython = get_ipython()\n",
    "if not hasattr(ipython._showtraceback, \"NOTEBOOK_EXCEPTION_FLAG\"):\n",
    "\n",
    "    def __exception_handler(exception_class, message, traceback):\n",
    "        if issubclass(exception_class, NotebookException):\n",
    "            print(termcolor.colored(f\"ERROR:  {message}\", \"red\"), file=sys.stderr)\n",
    "        else:\n",
    "            __exception_handler.old_showtraceback(exception_class, message, traceback)\n",
    "\n",
    "    __exception_handler.old_showtraceback = ipython._showtraceback\n",
    "    __exception_handler.NOTEBOOK_EXCEPTION_FLAG = True\n",
    "    ipython._showtraceback = __exception_handler\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" not in os.environ:\n",
    "    raise NotebookException(\n",
    "        \"\\nThis notebook only works in a TPU instance.\\nGo to Runtime > Change runtime type and change Hardware accelerator to TPU.\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"Installing mesh-transformer-jax...\", \"magenta\"))\n",
    "%cd /content\n",
    "!rm -r mesh-transformer-jax/\n",
    "# You can use the official mesh-transformer-jax if you're going to tune a\n",
    "# GPT-J-6B model, but you must use this fork if you want to tune any other type\n",
    "# of model.\n",
    "!git clone https://github.com/VE-FORBRYDERNE/mesh-transformer-jax\n",
    "!pip install -r mesh-transformer-jax/requirements.txt\n",
    "!pip install mesh-transformer-jax/ jax==0.2.12 torch progressbar2 ftfy\n",
    "!pip uninstall -y optax\n",
    "!pip install -U git+git://github.com/deepmind/optax.git@00c3256466196d0f6c523a8854323b08ef534600\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"\\nInitializing JAX...\\n\", \"magenta\"))\n",
    "import requests  # Only for connecting to Colab TPU and for nothing else\n",
    "import progressbar\n",
    "from tqdm.auto import tqdm\n",
    "import IPython\n",
    "import multiprocessing\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import uuid\n",
    "import base64\n",
    "import ftfy\n",
    "import zipfile\n",
    "import google.colab\n",
    "from typing import Iterable, List, Optional\n",
    "import io\n",
    "import pathlib\n",
    "import math\n",
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import haiku as hk\n",
    "import optax\n",
    "import torch\n",
    "import mesh_transformer\n",
    "import mesh_transformer.util\n",
    "import mesh_transformer.layers\n",
    "import mesh_transformer.transformer_shard\n",
    "\n",
    "\n",
    "seq = 2048\n",
    "\n",
    "starting_step = -1\n",
    "\n",
    "\n",
    "if not os.path.isdir(\".notebook_pickle\"):\n",
    "    os.mkdir(\".notebook_pickle\")\n",
    "\n",
    "# Required for certain optax optimizers to work properly with haiku modules\n",
    "# as per https://github.com/deepmind/dm-haiku/issues/191\n",
    "os.environ[\"HAIKU_FLATMAPPING\"] = \"0\"\n",
    "\n",
    "# In JAX 0.2.13, jax.tree_multimap was renamed to jax.tree_map and\n",
    "# jax.tree_multimap became an alias of this new jax.tree_map,\n",
    "# and optax depends on this change, so we're shimming jax.tree_map calls to\n",
    "# go to the current jax.tree_multimap (which is the same as the JAX 0.2.13\n",
    "# versions of both jax.tree_map and jax.tree_multimap) for compatibility\n",
    "# with optax.\n",
    "jax.tree_map = jax.tree_multimap\n",
    "\n",
    "\n",
    "# Colab doesn't seem to like ReplicatedLayerNorm, so we're just going to use\n",
    "# haiku's standard LayerNorm modules, which we can do because we aren't going\n",
    "# to train any layernorm parameters.\n",
    "old_getnorm = mesh_transformer.layers.getnorm\n",
    "\n",
    "\n",
    "def getnorm(type: str):\n",
    "    if type == \"layernorm\":\n",
    "        return hk.LayerNorm(-1, True, True, name=\"replicated_layer_norm\")\n",
    "    elif type == \"layernorm-nobias\":\n",
    "        return hk.LayerNorm(-1, True, False, name=\"replicated_layer_norm\")\n",
    "    else:\n",
    "        return old_getnorm(type)\n",
    "\n",
    "\n",
    "mesh_transformer.layers.getnorm = getnorm\n",
    "\n",
    "\n",
    "def shatter(in_axes: str, out_axes: str):\n",
    "    \"\"\"Helper function for setting up JAX xmaps.\n",
    "\n",
    "    This is a decorator that creates an xmapped version of a function.\n",
    "    Your function's arguments should be NumPy arrays or JAX pytrees with\n",
    "    NumPy arrays at the leaves.  Your actual function will be run 8 times in\n",
    "    parallel (once for each TPU core).  You can specify for some of your\n",
    "    function's arguments to be sharded (using a different value for that\n",
    "    argument on each TPU).  Sharded arguments will be split along the leading\n",
    "    dimension into 8 subarrays, for example if your sharded argument is an\n",
    "    array with shape (8, 10, 2), each of the 8 versions of your function will\n",
    "    receive one subarray with shape (10, 2).  If the leading dimension of your\n",
    "    sharded arguments aren't equal to 8, you will receive an error.\n",
    "    The return value(s) of your function can also be sharded, which will\n",
    "    result in each of the 8 values from your 8 versions of your function to be\n",
    "    concatenated together along a new leading axis.  Non-sharded arguments\n",
    "    should have a leading axis of size 1.\n",
    "\n",
    "    Note: Your function must have at least one sharded argument AND one\n",
    "    non-sharded argument, otherwise an error will be thrown.  Also your\n",
    "    function shouldn't have any default arguments, *args or **kwargs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_axes : str\n",
    "        A string with the same length as the number of parameters your function\n",
    "        has, where each character of the string is 's' or 'b'.  's' means the\n",
    "        corresponding parameter of your function should be sharded; 'b' means\n",
    "        the corresponding parameter of your function should not be sharded.\n",
    "    out_axes : str\n",
    "        A string with the same length as the number of returns your function\n",
    "        has, where each character of the string is 's' or 'b'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Callable[Callable[..., Any], Callable[..., Any]]\n",
    "        A function that takes one argument (the function that you want to be\n",
    "        xmapped) and returns the xmapped version of your function.\n",
    "    \"\"\"\n",
    "    in_axes = tuple(map(lambda c: [\"batch\" if c == \"b\" else \"shard\", ...], in_axes))\n",
    "    out_axes = tuple(map(lambda c: [\"batch\" if c == \"b\" else \"shard\", ...], out_axes))\n",
    "    if len(in_axes) == 1:\n",
    "        in_axes = in_axes[0]\n",
    "    if len(out_axes) == 1:\n",
    "        out_axes = out_axes[0]\n",
    "    return lambda fun: jax.experimental.maps.xmap(\n",
    "        fun=fun,\n",
    "        in_axes=in_axes,\n",
    "        out_axes=out_axes,\n",
    "        donate_argnums=(0,),\n",
    "        axis_resources={\"shard\": \"mp\", \"batch\": \"dp\"},\n",
    "    )\n",
    "\n",
    "\n",
    "class EmbeddingShard(mesh_transformer.transformer_shard.EmbeddingShard):\n",
    "    \"\"\"\n",
    "    A version of Mesh Transformer JAX's EmbeddingShard with a trainable\n",
    "    soft prompt module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: dict, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "        self.softtune_in_dim = soft_in_dim\n",
    "        self.softtune_in_dim_per_shard = math.ceil(\n",
    "            self.softtune_in_dim / config[\"cores_per_replica\"]\n",
    "        )\n",
    "        self.softtune_proj = hk.Linear(\n",
    "            self.out_dim,\n",
    "            w_init=hk.initializers.TruncatedNormal(\n",
    "                stddev=1 / np.sqrt(self.softtune_in_dim)\n",
    "            ),\n",
    "            with_bias=False,\n",
    "            name=\"softtune_linear\",\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jnp.array, **kwargs) -> jnp.array:\n",
    "        pe_length = kwargs.get(\"pe_length\", 0)\n",
    "        shard_start_index = jax.lax.axis_index(\"shard\") * self.in_dim_per_shard\n",
    "        proj_out = self.proj(\n",
    "            jax.nn.one_hot(x - shard_start_index, self.in_dim_per_shard)\n",
    "        )\n",
    "        mask = jnp.broadcast_to((x < self.in_dim)[:, jnp.newaxis], proj_out.shape)\n",
    "        proj_out = jnp.where(mask, proj_out, 0)\n",
    "        if (\n",
    "            not kwargs.get(\"mtj_softtuner_disable_pe\", False)\n",
    "            and self.positional_embeddings is not None\n",
    "        ):\n",
    "            pe_length = jnp.int32(pe_length)\n",
    "            shard_roll_index = jnp.int32(\n",
    "                jax.lax.axis_index(\"shard\") * self.out_dim_per_shard\n",
    "            )\n",
    "            pos_embed = jnp.pad(\n",
    "                self.positional_embeddings,\n",
    "                ((0, 0), (0, self.out_dim - self.out_dim_per_shard)),\n",
    "            )\n",
    "            pos_embed = jnp.roll(pos_embed, shard_roll_index, axis=1)\n",
    "            pos_embed = jnp.roll(pos_embed, -pe_length, axis=0)[-proj_out.shape[0] :]\n",
    "            proj_out += pos_embed\n",
    "        soft_shard_start_index = (\n",
    "            jax.lax.axis_index(\"shard\") * self.softtune_in_dim_per_shard\n",
    "        )\n",
    "        proj_out += self.softtune_proj(\n",
    "            jax.nn.one_hot(\n",
    "                x - soft_shard_start_index - self.in_dim, self.softtune_in_dim_per_shard\n",
    "            )\n",
    "        )\n",
    "        return mesh_transformer.util.g_psum(proj_out)\n",
    "\n",
    "\n",
    "mesh_transformer.transformer_shard.EmbeddingShard = EmbeddingShard\n",
    "\n",
    "\n",
    "class EmbeddingCausalTransformer(mesh_transformer.transformer_shard.CausalTransformer):\n",
    "    \"\"\"\n",
    "    A version of Mesh Transformer JAX's CausalTransformer with a function for\n",
    "    embedding a 1D array of token IDs and returning the embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "        @shatter(\"sb\", \"b\")\n",
    "        def _get_embedding_matrix(params: dict, tokens: jnp.array) -> jnp.array:\n",
    "            @hk.without_apply_rng\n",
    "            @hk.transform\n",
    "            def inner(tokens: jnp.array):\n",
    "                transformer = mesh_transformer.transformer_shard.CausalTransformerShard(\n",
    "                    self.config\n",
    "                )\n",
    "                return transformer.embed(tokens, mtj_softtuner_disable_pe=True)\n",
    "\n",
    "            return inner.apply(params, tokens)\n",
    "\n",
    "        self._get_embedding_matrix = _get_embedding_matrix\n",
    "\n",
    "    def get_embedding_matrix(self, tokens: np.array) -> jnp.array:\n",
    "        \"\"\"Embeds the given array of tokens.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : numpy.array\n",
    "            A 1-dimensional NumPy/jax.numpy array with dtype `numpy.uint32` or\n",
    "            `jax.numpy.uint32` containing the IDs of the tokens you want to\n",
    "            embed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        jax.numpy.array\n",
    "            Embedding matrix for your tokens as a 2-dimensional jax.numpy array\n",
    "            with dtype `jax.numpy.float32` and shape `(len(tokens), d_model)`,\n",
    "            where `d_model` is the embedding dimension (or \"model dimension\")\n",
    "            of your model.\n",
    "        \"\"\"\n",
    "        return self._get_embedding_matrix(\n",
    "            self.state[\"params\"],\n",
    "            tokens[np.newaxis, :],\n",
    "        )[0]\n",
    "\n",
    "\n",
    "def read_ckpt_custom(pytree, dir: str, shards_in: int, load_opt: bool = True):\n",
    "    \"\"\"Loads the model's state from a checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pytree\n",
    "        State of the network (network.state).\n",
    "    dir : str\n",
    "        Path to the model checkpoint.  Must contain a trailing slash.\n",
    "    shards_in : int\n",
    "        Number of shards the model is broken up into.  Should be 8.\n",
    "    load_opt : bool, default=True\n",
    "        Whether or not to load the optimizer state from the model checkpoint\n",
    "        if it has one.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        Updated version of network.state.\n",
    "    spmodule : str\n",
    "        The trainable soft prompt module's module name, so that you can change\n",
    "        the embedding matrix for the trainable soft prompt (assuming your\n",
    "        embedding matrix is called `soft_embeddings`) by doing\n",
    "        `network.state[\"params\"][spmodule][\"w\"] = soft_embeddings`.\n",
    "    \"\"\"\n",
    "    pieces = 16\n",
    "\n",
    "    old_flattened, structure = jax.tree_flatten(pytree)\n",
    "\n",
    "    soft_embeddings_mask, _ = jax.tree_flatten(\n",
    "        hk.data_structures.map(\n",
    "            lambda module_name, name, value: module_name.split(\"/~/\", 3)[-1]\n",
    "            == \"softtune_linear\",\n",
    "            pytree[\"params\"],\n",
    "        )\n",
    "    )\n",
    "    assert sum(soft_embeddings_mask) == 1\n",
    "\n",
    "    desync_mask, _ = jax.tree_flatten(\n",
    "        hk.data_structures.map(\n",
    "            lambda module_name, name, value: module_name.split(\"/~/\", 3)[-1].startswith(\n",
    "                \"replicated_layer_norm\"\n",
    "            ),\n",
    "            pytree[\"params\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    original_opt_state = pytree[\"opt_state\"]\n",
    "\n",
    "    n_tensors = 0\n",
    "    for file_index in range(pieces):\n",
    "        n_tensors += len(np.load(f\"{dir}shard_0/{file_index}.npz\").keys())\n",
    "\n",
    "    def _unshard(bar):\n",
    "        unsharded = []\n",
    "        tensor_index = progress_index = 0\n",
    "\n",
    "        for file_index in range(pieces):\n",
    "            array_keys = [*np.load(f\"{dir}shard_0/{file_index}.npz\").keys()]\n",
    "            for array_index in range(len(array_keys)):\n",
    "                unstacked = []\n",
    "                for shard_index in range(shards_in):\n",
    "                    if (\n",
    "                        tensor_index < len(desync_mask)\n",
    "                        and desync_mask[tensor_index]\n",
    "                        and shard_index > 0\n",
    "                    ):\n",
    "                        continue\n",
    "                    if (\n",
    "                        tensor_index < len(soft_embeddings_mask)\n",
    "                        and soft_embeddings_mask[tensor_index]\n",
    "                    ):\n",
    "                        unsharded.append(\n",
    "                            jnp.empty(\n",
    "                                (\n",
    "                                    shards_in,\n",
    "                                    math.ceil(soft_in_dim / shards_in),\n",
    "                                    params[\"d_model\"],\n",
    "                                ),\n",
    "                                dtype=jnp.float32,\n",
    "                            )\n",
    "                        )\n",
    "                        tensor_index += 1\n",
    "\n",
    "                    npz = np.load(f\"{dir}shard_{shard_index}/{file_index}.npz\")\n",
    "                    array = npz[array_keys[array_index]]\n",
    "                    if array.dtype == \"V2\":\n",
    "                        array.dtype = jnp.bfloat16\n",
    "                    unstacked.append(array)\n",
    "\n",
    "                if tensor_index < len(desync_mask) and desync_mask[tensor_index]:\n",
    "                    x = network.move_xmap(\n",
    "                        jnp.tile(unstacked[0], (shards_in, 1)),\n",
    "                        np.zeros(params[\"cores_per_replica\"]),\n",
    "                    )\n",
    "                else:\n",
    "                    x = network.move_xmap(\n",
    "                        jnp.stack(unstacked),\n",
    "                        np.zeros(params[\"cores_per_replica\"]),\n",
    "                    )\n",
    "                unsharded.append(x)\n",
    "\n",
    "                bar.update(progress_index)\n",
    "\n",
    "                assert (\n",
    "                    x.shape == old_flattened[tensor_index].shape\n",
    "                ), f\"Incompatible checkpoints {x.shape} vs {old_flattened[tensor_index].shape}\"\n",
    "                progress_index += 1\n",
    "                tensor_index += 1\n",
    "\n",
    "        return unsharded\n",
    "\n",
    "    print(\n",
    "        \"\\nPlease wait while we load the model's tensors into the TPU memory.\",\n",
    "        flush=True,\n",
    "    )\n",
    "    with progressbar.ProgressBar(\n",
    "        max_value=n_tensors,\n",
    "        widgets=[\n",
    "            progressbar.AnimatedMarker(\n",
    "                \"⡀⡁⡂⡃⡄⡅⡆⡇⡈⡉⡊⡋⡌⡍⡎⡏⡐⡑⡒⡓⡔⡕⡖⡗⡘⡙⡚⡛⡜⡝⡞⡟⡠⡡⡢⡣⡤⡥⡦⡧⡨⡩⡪⡫⡬⡭⡮⡯⡰⡱⡲⡳⡴⡵⡶⡷⡸⡹⡺⡻⡼⡽⡾⡿⢀⢁⢂⢃⢄⢅⢆⢇⢈⢉⢊⢋⢌⢍⢎⢏⢐⢑⢒⢓⢔⢕⢖⢗⢘⢙⢚⢛⢜⢝⢞⢟⢠⢡⢢⢣⢤⢥⢦⢧⢨⢩⢪⢫⢬⢭⢮⢯⢰⢱⢲⢳⢴⢵⢶⢷⢸⢹⢺⢻⢼⢽⢾⢿⣀⣁⣂⣃⣄⣅⣆⣇⣈⣉⣊⣋⣌⣍⣎⣏⣐⣑⣒⣓⣔⣕⣖⣗⣘⣙⣚⣛⣜⣝⣞⣟⣠⣡⣢⣣⣤⣥⣦⣧⣨⣩⣪⣫⣬⣭⣮⣯⣰⣱⣲⣳⣴⣵⣶⣷⣸⣹⣺⣻⣼⣽⣾⣿\"\n",
    "            ),\n",
    "            \"  \",\n",
    "            progressbar.ETA(),\n",
    "            \"   \",\n",
    "            progressbar.Counter(),\n",
    "            f\"/{n_tensors}  \",\n",
    "            progressbar.Percentage(),\n",
    "            \"  \",\n",
    "            progressbar.Bar(left=\"[\", right=\"]\", marker=\"█\"),\n",
    "        ],\n",
    "    ) as bar:\n",
    "        try:\n",
    "            unsharded = _unshard(bar)\n",
    "        except AssertionError:\n",
    "            load_opt = False  # no opt to load in ckpt\n",
    "            del pytree[\"opt_state\"]\n",
    "            old_flattened, structure = jax.tree_flatten(pytree)\n",
    "            unsharded = _unshard(bar)\n",
    "\n",
    "    loaded_pytree = jax.tree_unflatten(structure, unsharded)\n",
    "\n",
    "    print(\"\\nFinished loading the model!\\n\\n\\n\")\n",
    "\n",
    "    if not load_opt:\n",
    "        loaded_pytree[\"opt_state\"] = original_opt_state\n",
    "    return loaded_pytree, next(\n",
    "        state\n",
    "        for state in loaded_pytree[\"params\"]\n",
    "        if state.split(\"/~/\", 3)[-1] == \"softtune_linear\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Training functions\n",
    "\n",
    "\n",
    "@shatter(\"sb\", \"s\")\n",
    "def _init_opt_state(params: dict, aux: jnp.array):\n",
    "    return mesh_transformer.util.to_f32(network.config[\"optimizer\"].init(params))\n",
    "\n",
    "\n",
    "def init_opt_state(params: dict):\n",
    "    \"\"\"Initializes optax state for the given haiku module parameters\"\"\"\n",
    "    return _init_opt_state(params, np.empty(1))\n",
    "\n",
    "\n",
    "def train_grad(state, ctx, tgt):\n",
    "    \"\"\"\n",
    "    This function converts the model into a mathematical function with 6\n",
    "    billion arguments (one for each parameter the model has) that returns a\n",
    "    single number, the \"loss\" (lower loss is better), and then calculates the\n",
    "    gradient of that function.  Of course, the gradient of a function with 6\n",
    "    billion arguments is a vector of length 6 billion, and that's just a waste\n",
    "    of memory given that we're only training the soft prompt and not the entire\n",
    "    model, so we only keep the parts of the gradient that are from the\n",
    "    soft prompt module.\n",
    "    \"\"\"\n",
    "\n",
    "    @hk.without_apply_rng\n",
    "    @hk.transform\n",
    "    def inner(ctx, tgt):\n",
    "        \"\"\"\n",
    "        This is the function that we're going to take the gradient of.\n",
    "        \"\"\"\n",
    "        transformer = mesh_transformer.transformer_shard.CausalTransformerShard(\n",
    "            network.config\n",
    "        )\n",
    "        out = transformer.loss(ctx, tgt, z_loss=True)\n",
    "        return out[\"loss\"], out[\"last_loss\"]\n",
    "\n",
    "    # Compute gradient and also the actual loss value\n",
    "    val_grad_fn = jax.value_and_grad(inner.apply, has_aux=True)\n",
    "    (loss, last_loss), grad = val_grad_fn(\n",
    "        mesh_transformer.util.to_bf16(state[\"params\"]), ctx, tgt\n",
    "    )\n",
    "    # Remove everything from the gradient that isn't related to the\n",
    "    # soft prompt\n",
    "    grad = grad[spmodule]\n",
    "    # Calculate the Euclidean norm of the modified gradient\n",
    "    gnorm = mesh_transformer.util.global_norm(grad)\n",
    "    # Return the modified gradient, the loss and last loss, and the\n",
    "    # norm of the modified gradient\n",
    "    return grad, loss, last_loss, gnorm\n",
    "\n",
    "\n",
    "@shatter(\"sbb\", \"bbbb\")\n",
    "def train_initial(state, ctx, tgt):\n",
    "    \"\"\"\n",
    "    This function just runs train_grad.\n",
    "    \"\"\"\n",
    "    grad, loss, last_loss, gnorm = train_grad(state, ctx, tgt)\n",
    "    return grad, loss, last_loss, gnorm\n",
    "\n",
    "\n",
    "@shatter(\"sbbb\", \"bbbb\")\n",
    "def train_intermediate(state, old_grad, ctx, tgt):\n",
    "    \"\"\"\n",
    "    This function runs train_grad and then adds old_grad to the gradient vector\n",
    "    it returns.\n",
    "    \"\"\"\n",
    "    grad, loss, last_loss, gnorm = train_grad(state, ctx, tgt)\n",
    "    grad = jax.tree_multimap(lambda a, b: a + b, old_grad, grad)\n",
    "    return grad, loss, last_loss, gnorm\n",
    "\n",
    "\n",
    "@shatter(\"sbb\", \"bbs\")\n",
    "def train_final(state, grad, gnorm):\n",
    "    \"\"\"\n",
    "    This function takes the gradient accumulated by train_intermediate,\n",
    "    applies the Adam algorithm to it and then adds (element-wise) the\n",
    "    result to the (flattened) soft prompt tensor.\n",
    "    \"\"\"\n",
    "    grad_norm_micro = jax.lax.pmean(gnorm, \"batch\")\n",
    "    grad = jax.lax.pmean(grad, \"batch\")\n",
    "    grad_norm = mesh_transformer.util.global_norm(grad)\n",
    "    # Apply Adam algorithm to grad to get updates (output of the Adam\n",
    "    # algorithm) and new_opt_state (new state of the Adam optimizer)\n",
    "    updates, new_opt_state = network.config[\"optimizer\"].update(\n",
    "        grad, state[\"opt_state\"], params=state[\"params\"][spmodule]\n",
    "    )\n",
    "    # optax.apply_updates here just returns the element-wise sum\n",
    "    # of state[\"params\"][spmodule] and updates, cast to bfloat16.\n",
    "    state[\"params\"][spmodule] = optax.apply_updates(\n",
    "        state[\"params\"][spmodule], mesh_transformer.util.to_f32(updates)\n",
    "    )\n",
    "    return (\n",
    "        grad_norm / gradient_accumulation_steps,\n",
    "        grad_norm_micro,\n",
    "        {\n",
    "            \"params\": state[\"params\"],\n",
    "            \"step\": state[\"step\"] + 1,\n",
    "            \"opt_state\": new_opt_state,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def show_spinner() -> multiprocessing.Process:\n",
    "    \"\"\"\n",
    "    Shows a bouncing progress bar.  To stop it, save the return value of this\n",
    "    function as (for example) `spinner`, and then run spinner.terminate().\n",
    "    \"\"\"\n",
    "\n",
    "    def _show_spinner():\n",
    "        bar = progressbar.ProgressBar(\n",
    "            max_value=progressbar.UnknownLength,\n",
    "            widgets=[\n",
    "                progressbar.Timer(),\n",
    "                \"  \",\n",
    "                progressbar.BouncingBar(left=\"[\", right=\"]\", marker=\"█\"),\n",
    "            ],\n",
    "        )\n",
    "        i = 0\n",
    "        while True:\n",
    "            bar.update(i)\n",
    "            time.sleep(0.1)\n",
    "            i += 1\n",
    "\n",
    "    spinner = multiprocessing.Process(target=_show_spinner, args=())\n",
    "    spinner.start()\n",
    "    return spinner\n",
    "\n",
    "\n",
    "def save_variable(name: str, val) -> None:\n",
    "    \"\"\"Save a variable so it can be restored later with `restore_variable`.\"\"\"\n",
    "    with open(\".notebook_pickle/\" + name, \"wb\") as f:\n",
    "        pickle.dump(val, f)\n",
    "\n",
    "\n",
    "def restore_variable(name: str) -> None:\n",
    "    \"\"\"Restore a variable saved with `save_variable` if it exists.\"\"\"\n",
    "    if not os.path.exists(\".notebook_pickle/\" + name):\n",
    "        return\n",
    "    with open(\".notebook_pickle/\" + name, \"rb\") as f:\n",
    "        globals()[name] = pickle.load(f)\n",
    "\n",
    "\n",
    "def spform_callback(form_input: str):\n",
    "    \"\"\"\n",
    "    This function gets called when we click the Submit button in the cell\n",
    "    that asks you for your initial soft prompt\n",
    "    \"\"\"\n",
    "    max_tokenized_len = seq - 1\n",
    "    global initial_softprompt, step, starting_step, soft_in_dim\n",
    "    tokenized_input: List[int] = tokenizer.encode(\n",
    "        form_input, max_length=int(2e9), truncation=True\n",
    "    )\n",
    "    if len(tokenized_input) == 0:\n",
    "        initial_softprompt = soft_in_dim = None\n",
    "        del initial_softprompt\n",
    "        del soft_in_dim\n",
    "        starting_step = step = -1\n",
    "        return \"ERROR:  Your initial soft prompt cannot be empty!\"\n",
    "    if len(tokenized_input) >= max_tokenized_len:\n",
    "        initial_softprompt = soft_in_dim = None\n",
    "        del initial_softprompt\n",
    "        del soft_in_dim\n",
    "        starting_step = step = -1\n",
    "        return f\"ERROR:  Your initial soft prompt is too long!<br/>It is {len(tokenized_input)} tokens long,<br/>more than the maximum of {max_tokenized_len}.\"\n",
    "    initial_softprompt = tokenized_input\n",
    "    starting_step = step = 0\n",
    "    soft_in_dim = len(tokenized_input)\n",
    "    save_variable(\"initial_softprompt\", initial_softprompt)\n",
    "    save_variable(\"starting_step\", starting_step)\n",
    "    save_variable(\"soft_in_dim\", soft_in_dim)\n",
    "    return f\"Initial soft prompt set successfully!<br/>({len(tokenized_input)} token{'' if len(tokenized_input) == 1 else 's'} long)\"\n",
    "\n",
    "\n",
    "google.colab.output.register_callback(\"spform_callback\", spform_callback)\n",
    "\n",
    "\n",
    "\"\"\"HTML for the plots that get shown during training\"\"\"\n",
    "plot_html = \"\"\"\n",
    "    <style>\n",
    "        .row { display: flex; }\n",
    "        .col { flex: 1; }\n",
    "    </style>\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col\"><canvas id=\"plotl\"></canvas></div>\n",
    "        <div class=\"col\"><canvas id=\"plotg\"></canvas></div>\n",
    "        <div class=\"col\"><canvas id=\"plotr\"></canvas></div>\n",
    "    </div>\n",
    "    <script src=\"https://cdn.jsdelivr.net/npm/chart.js@3.5.1/dist/chart.min.js\" integrity=\"sha256-bC3LCZCwKeehY6T4fFi9VfOU0gztUa+S4cnkIhVPZ5E=\" crossorigin=\"anonymous\"></script>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeral.js/2.0.6/numeral.min.js\" integrity=\"sha512-USPCA7jmJHlCNRSFwUFq3lAm9SaOjwG8TaB8riqx3i/dAJqhaYilVnaf2eVUH5zjq89BU6YguUuAno+jpRvUqA==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n",
    "    <script>\n",
    "        var labels = [];\n",
    "        var plotl = new Chart(document.getElementById(\"plotl\").getContext(\"2d\"), {\n",
    "            type: 'line',\n",
    "            data: {\n",
    "                labels: labels,\n",
    "                datasets: [{\n",
    "                    label: 'Training Loss',\n",
    "                    borderColor: 'rgb(239, 41, 41)',\n",
    "                    cubicInterpolationMode: 'monotone',\n",
    "                    tension: 0.4,\n",
    "                    data: []\n",
    "                }]\n",
    "            },\n",
    "            options: {\n",
    "                animation: { duration: 0 },\n",
    "                elements: { point: { radius: 0 } },\n",
    "                scales: { x: { display: true, }, y: { display: true } },\n",
    "                interaction: { intersect: false, mode: 'nearest', axis: 'x' }\n",
    "            }\n",
    "        });\n",
    "        var plotg = new Chart(document.getElementById(\"plotg\").getContext(\"2d\"), {\n",
    "            type: 'line',\n",
    "            data: {\n",
    "                labels: labels,\n",
    "                datasets: [{\n",
    "                    label: 'Gradient L2 Norm',\n",
    "                    borderColor: 'rgb(114, 159, 207)',\n",
    "                    cubicInterpolationMode: 'monotone',\n",
    "                    tension: 0.4,\n",
    "                    data: []\n",
    "                }]\n",
    "            },\n",
    "            options: {\n",
    "                animation: { duration: 0 },\n",
    "                elements: { point: { radius: 0 } },\n",
    "                scales: { x: { display: true, }, y: { display: true, type: 'logarithmic' } },\n",
    "                interaction: { intersect: false, mode: 'nearest', axis: 'x' }\n",
    "            }\n",
    "        });\n",
    "        var plotr = new Chart(document.getElementById(\"plotr\").getContext(\"2d\"), {\n",
    "            type: 'line',\n",
    "            data: {\n",
    "                labels: labels,\n",
    "                datasets: [{\n",
    "                    label: 'Learning Rate',\n",
    "                    borderColor: 'rgb(173, 127, 168)',\n",
    "                    cubicInterpolationMode: 'monotone',\n",
    "                    tension: 0.4,\n",
    "                    data: []\n",
    "                }]\n",
    "            },\n",
    "            options: {\n",
    "                animation: { duration: 0 },\n",
    "                elements: { point: { radius: 0 } },\n",
    "                scales: { x: { display: true, }, y: { display: true } },\n",
    "                interaction: { intersect: false, mode: 'nearest', axis: 'x' },\n",
    "                plugins: {\n",
    "                    tooltip: {\n",
    "                        callbacks: {\n",
    "                            label: function(context) {\n",
    "                                return numeral(context.parsed.y).format('0.000e+0');\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        });\n",
    "\n",
    "        function push(label, l, g, r) {\n",
    "            labels.push(label);\n",
    "            plotl.data.datasets[0].data.push(l);\n",
    "            plotg.data.datasets[0].data.push(g);\n",
    "            plotr.data.datasets[0].data.push(r);\n",
    "        }\n",
    "\n",
    "        function update() {\n",
    "            plotl.update();\n",
    "            plotg.update();\n",
    "            plotr.update();\n",
    "        }\n",
    "    </script>\"\"\"\n",
    "\n",
    "\n",
    "# Restore any variables that may have been lost by restarting the instance\n",
    "for var in (\n",
    "    \"starting_step\",\n",
    "    \"params\",\n",
    "    \"ckpt_path\",\n",
    "    \"save_file\",\n",
    "    \"stparams\",\n",
    "    \"initial_softprompt\",\n",
    "    \"soft_in_dim\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"dataset_file\",\n",
    "    \"num_sequences\",\n",
    "):\n",
    "    restore_variable(var)\n",
    "\n",
    "\n",
    "print(\n",
    "    termcolor.colored(\"\\n\\nConnecting to your Colab instance's TPU...\", \"magenta\"),\n",
    "    flush=True,\n",
    ")\n",
    "spinner = show_spinner()\n",
    "colab_tpu_addr = os.environ[\"COLAB_TPU_ADDR\"].split(\":\")[0]\n",
    "requests.post(f\"http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607\")\n",
    "jax.config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "jax.config.FLAGS.jax_backend_target = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
    "spinner.terminate()\n",
    "if jax.device_count() < 8:\n",
    "    raise NotebookException(\n",
    "        \"We couldn't detect your Colab instance's TPU.\\nTry restarting the runtime (Runtime > Restart Runtime) and trying again.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if \"params\" in globals():\n",
    "    mesh_shape = (1, params[\"cores_per_replica\"])\n",
    "    devices = np.array(jax.devices()[: params[\"cores_per_replica\"]]).reshape(mesh_shape)\n",
    "    thread_resources_env = jax.experimental.maps.ResourceEnv(\n",
    "        jax.experimental.maps.Mesh(devices, (\"dp\", \"mp\"))\n",
    "    )\n",
    "    jax.experimental.maps.thread_resources.env = thread_resources_env\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"\\n\\nInitializing transformers...\", \"magenta\"))\n",
    "import transformers\n",
    "\n",
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"\\n\\nDone.\\n\\n\", \"green\"), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQV0iIUQwBI"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h14rXKs0Ujpe"
   },
   "source": [
    "# 2. Download the model or log in to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFRX357Uwi8"
   },
   "source": [
    "First we have to download and extract the model into your Colab instance, if you don't already have the model *unextracted* in your Google Drive. If you do have it unextracted in your Google Drive, you can skip this cell.\n",
    "\n",
    "You might want to look at the rest of the notebook while the model is downloading/extracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3xeg1twVDh1"
   },
   "outputs": [],
   "source": [
    "# Feel free to modify this cell to download a finetuned GPT-J-6B model instead.\n",
    "# You can also use GPT-Neo-2.7B models after first converting them with this\n",
    "# notebook:\n",
    "# https://colab.research.google.com/github/VE-FORBRYDERNE/mesh-transformer-jax/blob/modelcompat/convert_neo_pytorch_model_to_jax.ipynb\n",
    "\n",
    "\n",
    "print(termcolor.colored(\"Installing pv and zstd...\", \"magenta\"))\n",
    "# The official version of pv doesn't work in Colab anymore for some reason.\n",
    "# This fork contains a small patch to address the issue.\n",
    "!git clone https://github.com/VE-FORBRYDERNE/pv\n",
    "%cd pv\n",
    "!./configure\n",
    "!make\n",
    "!make install\n",
    "%cd ..\n",
    "!apt install zstd\n",
    "\n",
    "print(\n",
    "    termcolor.colored(\n",
    "        \"\\nDownloading GPT-J-6B model into your Colab instance...\", \"magenta\"\n",
    "    )\n",
    ")\n",
    "!wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
    "print(termcolor.colored(\"\\nExtracting the model...\", \"magenta\"))\n",
    "!pv step_383500_slim.tar.zstd | tar -I zstd -x\n",
    "!rm step_383500_slim.tar.zstd\n",
    "\n",
    "print(termcolor.colored(\"\\nDone.\\n\\n\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzkSBr8C8jFF"
   },
   "source": [
    "Are you using a GPT-J-6B or GPT-Neo-2.7B pretrained model? Use the dropdown below to select your model type and then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eyhAu7yJ8jFG"
   },
   "outputs": [],
   "source": [
    "model_type = \"GPT-J-6B\"  # @param [\"GPT-J-6B\", \"GPT-Neo-2.7B\"]\n",
    "\n",
    "if model_type == \"GPT-Neo-2.7B\":\n",
    "    params = {\n",
    "        \"compat\": \"neo\",\n",
    "        \"layers\": 32,\n",
    "        \"d_model\": 2560,\n",
    "        \"n_heads\": 20,\n",
    "        \"n_vocab\": 50257,\n",
    "        \"n_vocab_padding\": 143,\n",
    "        \"norm\": \"layernorm\",\n",
    "        \"pe\": \"fixed\",\n",
    "        \"seq\": seq,\n",
    "        \"cores_per_replica\": 4,\n",
    "    }\n",
    "else:\n",
    "    params = {\n",
    "        \"layers\": 28,\n",
    "        \"d_model\": 4096,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_vocab\": 50400,\n",
    "        \"norm\": \"layernorm\",\n",
    "        \"pe\": \"rotary\",\n",
    "        \"pe_rotary_dims\": 64,\n",
    "        \"seq\": seq,\n",
    "        \"cores_per_replica\": 8,\n",
    "    }\n",
    "assert (\n",
    "    params[\"cores_per_replica\"] > 1 and params[\"cores_per_replica\"] % 2 == 0\n",
    ") or params[\"cores_per_replica\"] == 1\n",
    "save_variable(\"params\", params)\n",
    "\n",
    "mesh_shape = (1, params[\"cores_per_replica\"])\n",
    "devices = np.array(jax.devices()[: params[\"cores_per_replica\"]]).reshape(mesh_shape)\n",
    "thread_resources_env = jax.experimental.maps.ResourceEnv(\n",
    "    jax.experimental.maps.Mesh(devices, (\"dp\", \"mp\"))\n",
    ")\n",
    "jax.experimental.maps.thread_resources.env = thread_resources_env\n",
    "\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywRUnnULWT99"
   },
   "source": [
    "If your model is stored unextracted in your Google Drive, you must run this cell below to allow us access to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2pZNzWtWgr7"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JeiYtrjWDs9"
   },
   "source": [
    "Type the path to the extracted model below and then run the cell below.\n",
    "\n",
    "If you just downloaded the normal GPT-J-6B model, then the default path that's already shown, `/content/step_383500`, is correct, so you just have to run the cell without changing the path.\n",
    "\n",
    "If you downloaded a finetuned model, you probably know where it is stored.\n",
    "\n",
    "If your model is in Google Drive, prefix your path with `/content/drive/MyDrive`. For example, if your model were stored in a directory in the root directory of your Google Drive called \"MLP\", the path would be `/content/drive/MyDrive/MLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "R8jrFmT9YN_q"
   },
   "outputs": [],
   "source": [
    "ckpt_path = \"/content/step_383500\"  # @param {type:\"string\"}\n",
    "\n",
    "ckpt_path = ckpt_path.replace(\"\\\\\", \"/\")\n",
    "if not ckpt_path.endswith(\"/\"):\n",
    "    ckpt_path += \"/\"\n",
    "\n",
    "if not os.path.isdir(ckpt_path):\n",
    "    del ckpt_path\n",
    "    raise NotebookException(\"That is not a path to a valid directory.\")\n",
    "if not os.path.exists(ckpt_path + \"shard_0/0.npz\"):\n",
    "    del ckpt_path\n",
    "    raise NotebookException(\"There doesn't seem to be a model in that directory.\")\n",
    "\n",
    "save_variable(\"ckpt_path\", ckpt_path)\n",
    "if \"network\" in globals():\n",
    "    print(\n",
    "        \"WARNING:  Due to memory constraints, you must restart your instance (Runtime > Restart runtime) before continuing further.\"\n",
    "    )\n",
    "    print(\"          (You also have to run Step 1 again)\")\n",
    "    del network\n",
    "    if \"spmodule\" in globals():\n",
    "        del spmodule\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmurZcInVuaj"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYbazEspYEVJ"
   },
   "source": [
    "# 3. Set up soft-tuning hyperparameters and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZR_v8ElI7zs"
   },
   "source": [
    "If you want to save your soft prompt into your Google Drive, run this cell to login to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0C70IkEI7Oy"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMu1kVgKDvEF"
   },
   "source": [
    "If you want to begin a new soft-tuning run, choose the path where we will save to. You will see a file there before the soft-tuning process is fully complete, it will be there so you can resume the soft-tuning process later if your Colab instance crashes. If you want to save into your Google Drive, prefix your path with `/content/drive/MyDrive`.\n",
    "\n",
    "If you want to resume a soft-tuning run for the aforementioned reason, choose the path to an existing MTJSP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hfV86njSEH6E"
   },
   "outputs": [],
   "source": [
    "save_file = \"/content/drive/MyDrive/my_softprompt.mtjsp\"  # @param {type:\"string\"}\n",
    "\n",
    "save_file = save_file.replace(\"\\\\\", \"/\")\n",
    "if save_file.endswith(\"/\"):\n",
    "    soft_in_dim = None\n",
    "    del soft_in_dim\n",
    "    del save_file\n",
    "    starting_step = step = -1\n",
    "    raise NotebookException(\"save_file should be a file, not a directory.\")\n",
    "\n",
    "os.makedirs(save_file.rsplit(\"/\", 1)[0].strip(), exist_ok=True)\n",
    "\n",
    "if os.path.exists(save_file):\n",
    "    try:\n",
    "        npz = np.load(save_file, allow_pickle=True)\n",
    "        assert npz[\"step\"] > 0\n",
    "        assert npz[\"tensor\"].ndim == 2 and \"opt_state\" in npz\n",
    "        assert npz[\"tensor\"].shape[0] < seq\n",
    "        assert npz[\"tensor\"].shape[1] == params[\"d_model\"]\n",
    "        assert all(\n",
    "            p in npz for p in (\"loss\", \"last_loss\", \"grad_norm\", \"grad_norm_micro\")\n",
    "        )\n",
    "        soft_in_dim = npz[\"tensor\"].shape[0]\n",
    "        starting_step = step = np.uint32(npz[\"step\"]).item()\n",
    "    except:\n",
    "        soft_in_dim = None\n",
    "        del soft_in_dim\n",
    "        del save_file\n",
    "        starting_step = step = -1\n",
    "        raise NotebookException(\"MTJSP file exists and is not a valid save file.\")\n",
    "    print(\"OK.\")\n",
    "    print(f\"We will resume soft-tuning at step {starting_step + 1}.\")\n",
    "    save_variable(\"starting_step\", starting_step)\n",
    "    save_variable(\"soft_in_dim\", soft_in_dim)\n",
    "else:\n",
    "    starting_step = step = -1\n",
    "    print(\"OK.\")\n",
    "    print(\"We will begin a new soft-tuning run.\")\n",
    "\n",
    "save_variable(\"save_file\", save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIXyab_-Zub7"
   },
   "source": [
    "If you are beginning a new soft-tuning run, choose a string to initialize your soft prompt with. It should be roughly 20-200 tokens long. The maximum allowed length is 2047 tokens. It's recommended that your string should end with two newline characters and have no other leading or trailing whitespace on any line.\n",
    "\n",
    "If you're resuming a soft-tuning run with an existing MTJSP file, you can skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j2DFnScvaNPY"
   },
   "outputs": [],
   "source": [
    "#@markdown ### Run this cell once to make a text box appear for you to type in your initial soft prompt.<br/>After that, press the \"Submit\" button underneath the text box; do not run this cell a second time.\n",
    "%%html\n",
    "<form>\n",
    "    <textarea id=\"softprompt\" rows=\"10\" cols=\"80\">Le Jeu du Prochain Train itself is simplicity in motion. The object: Be the last of your round's six to jump from one side of the tracks to the other - that is, across the tracks - before the train passes.\n",
    "\n",
    "</textarea>\n",
    "    <br/>\n",
    "    <p><input id=\"submit-softprompt\" type=\"button\" value=\"Submit\" /></p>\n",
    "</form>\n",
    "<br/>\n",
    "<p id=\"softprompt-message\"></p>\n",
    "<script type=\"text/javascript\">\n",
    "    (function() {\n",
    "        var submit = document.getElementById(\"submit-softprompt\");\n",
    "        var softprompt = document.getElementById(\"softprompt\");\n",
    "        var message = document.getElementById(\"softprompt-message\");\n",
    "        submit.addEventListener(\"click\", async function() {\n",
    "            var msg = await google.colab.kernel.invokeFunction(\"spform_callback\", [softprompt.value], {});\n",
    "            message.innerHTML = msg.data['text/plain'].replace(/(?:^ *'*)|(?:'* *$)/g, \"\");\n",
    "        });\n",
    "        softprompt.addEventListener(\"input\", function() {\n",
    "            message.innerHTML = \"\";\n",
    "        });\n",
    "    })();\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-z_9TbzIBAH"
   },
   "source": [
    "If your dataset is stored in Google Drive, you have to log in to Google Drive so we can access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nS_qrEkIbhp"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzcUxnmVIcdx"
   },
   "source": [
    "If your dataset is a single txt file or collection of txt files, we have to convert it to npy format first. If you have already used this notebook to convert your txt dataset to npy, you can skip this cell.\n",
    "\n",
    "`dataset_path` should be the path to either a single txt file or a folder with one or more txt files in it. Then run the cell, and we will make a npy file using your dataset at the given path (we will create the required directory tree for the output file if the output file's directory doesn't already exist). If your txt files are in Google Drive, prefix your path with `/content/drive/MyDrive`.\n",
    "\n",
    "`batch_size` is explained in this article: https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e. The maximum possible batch size is 2048 minus the number of tokens in your initial soft prompt, so if your initial soft prompt is 49 tokens long then the maximum allowed batch size is 1999. If your batch_size is too high, we will automatically lower it to the highest possible value, so just leave it at 2048 if you want us to do that. Epochs is the amount of times to repeat your dataset (it will be shuffled every time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rip64xGwRJQC"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/content/drive/MyDrive/dataset.txt\"  # @param {type:\"string\"}\n",
    "output_file = \"/content/drive/MyDrive/output.npy\"  # @param {type:\"string\"}\n",
    "batch_size = 2048  # @param {type:\"integer\"}\n",
    "epochs = 1  # @param {type:\"integer\"}\n",
    "\n",
    "dataset_path = dataset_path.replace(\"\\\\\", \"/\")\n",
    "output_file = output_file.replace(\"\\\\\", \"/\")\n",
    "if \"starting_step\" not in globals() or starting_step == -1:\n",
    "    del dataset_path\n",
    "    del output_file\n",
    "    del batch_size\n",
    "    del epochs\n",
    "    raise NotebookException(\n",
    "        \"You did not load from a MTJSP file or define an initial soft prompt.\"\n",
    "    )\n",
    "if not isinstance(batch_size, int) or batch_size < 1:\n",
    "    del dataset_path\n",
    "    del output_file\n",
    "    del batch_size\n",
    "    del epochs\n",
    "    raise NotebookException(\"batch_size must be an integer greater than zero.\")\n",
    "if not isinstance(epochs, int) or epochs < 1:\n",
    "    del dataset_path\n",
    "    del output_file\n",
    "    del batch_size\n",
    "    del epochs\n",
    "    raise NotebookException(\"epochs must be an integer greater than zero.\")\n",
    "if output_file.endswith(\"/\"):\n",
    "    del dataset_path\n",
    "    del output_file\n",
    "    del batch_size\n",
    "    raise NotebookException(\"output_file should be a file, not a directory.\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    del dataset_path\n",
    "    del output_file\n",
    "    del batch_size\n",
    "    del epochs\n",
    "    raise NotebookException(\"dataset_path is not set to a valid file or directory.\")\n",
    "\n",
    "\n",
    "batch_size = min(batch_size, seq - soft_in_dim)\n",
    "assert batch_size >= 0\n",
    "print(\n",
    "    termcolor.colored(\n",
    "        \"\\nIf you see a warning above about token indices, ignore it.  That warning is normal.\\n\",\n",
    "        \"magenta\",\n",
    "    )\n",
    ")\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(termcolor.colored(\"Tokenizing your dataset...\\n\", \"magenta\"))\n",
    "\n",
    "if os.path.isfile(dataset_path):\n",
    "    files = [dataset_path]\n",
    "else:\n",
    "    files = (\n",
    "        os.path.join(dataset_path, filename) for filename in os.listdir(dataset_path)\n",
    "    )\n",
    "tokens = []\n",
    "for path in files:\n",
    "    with open(path) as f:\n",
    "        tokens.extend(tokenizer.encode(ftfy.fix_text(f.read())) + [50256])\n",
    "\n",
    "print(\"Dataset size (in tokens):\", len(tokens))\n",
    "if len(tokens) < batch_size + 1:\n",
    "    raise NotebookException(\n",
    "        \"Your dataset is too small!  The number of tokens has to be greater than the batch size.\"\n",
    "    )\n",
    "tail = len(tokens) % (batch_size + 1)\n",
    "if tail:\n",
    "    print(\n",
    "        f\"We're removing the last {tail} tokens from your dataset to make the length a multiple of {batch_size+1}.\"\n",
    "    )\n",
    "    tokens = tokens[:-tail]\n",
    "\n",
    "tokens = np.array(tokens, dtype=np.uint16).reshape((-1, batch_size + 1))\n",
    "if epochs > 1:\n",
    "    rng = np.random.Generator(np.random.PCG64(1729))\n",
    "    tokens = np.concatenate(\n",
    "        (\n",
    "            tokens,\n",
    "            *(rng.permutation(tokens, axis=0) for i in range(epochs - 1)),\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "print(f\"Total sequences in your dataset: {tokens.shape[0]}\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    np.save(output_file, tokens)\n",
    "\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHkmYHWEfEiK"
   },
   "source": [
    "Here, we set the npy that we will use for soft-tuning.\n",
    "\n",
    "`gradient_accumulation_steps` is described here: https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa. It's preferable to have gradient accumulation steps in the 16-32 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1igD8Q3ZfL8M"
   },
   "outputs": [],
   "source": [
    "dataset_file = \"/content/drive/MyDrive/output.npy\"  # @param {type:\"string\"}\n",
    "gradient_accumulation_steps = 16  # @param {type:\"integer\"}\n",
    "\n",
    "save_variable(\"gradient_accumulation_steps\", gradient_accumulation_steps)\n",
    "\n",
    "if not isinstance(gradient_accumulation_steps, int) or gradient_accumulation_steps < 1:\n",
    "    del dataset_file\n",
    "    del gradient_accumulation_steps\n",
    "    raise NotebookException(\n",
    "        \"gradient_accumulation_steps must be an integer greater than zero.\"\n",
    "    )\n",
    "if not os.path.exists(dataset_file):\n",
    "    del dataset_file\n",
    "    del gradient_accumulation_steps\n",
    "    raise NotebookException(\"Could not find any file at that path.\")\n",
    "\n",
    "dataset = np.load(dataset_file, mmap_mode=\"r\")\n",
    "assert dataset.ndim >= 2\n",
    "assert dataset.shape[0] >= 2\n",
    "num_sequences = dataset.shape[0]\n",
    "print(\"Batch size of your dataset:\", dataset.shape[1] - 1)\n",
    "print(\"Total sequences in your dataset:\", num_sequences)\n",
    "print()\n",
    "\n",
    "if num_sequences < gradient_accumulation_steps:\n",
    "    del dataset_file\n",
    "    del gradient_accumulation_steps\n",
    "    del num_sequences\n",
    "    raise NotebookException(\n",
    "        \"Your dataset is too small!  gradient_accumulation_steps must be less than or equal to the number of sequences.\"\n",
    "    )\n",
    "\n",
    "if dataset.shape[1] - 1 > seq - soft_in_dim:\n",
    "    print(\n",
    "        f\"WARNING:  The batch size of your dataset is {dataset.shape[1] - 1}\\n\"\n",
    "        f\"which is larger than the allowed maximum of {seq - soft_in_dim}.\\n\"\n",
    "        \"Your dataset will be truncated!\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "save_variable(\"num_sequences\", num_sequences)\n",
    "save_variable(\"dataset_file\", dataset_file)\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbt6qrnx1Ku6"
   },
   "source": [
    "Now it is time to set the other soft-tuning hyperparameters. Edit the numbers below (or don't edit any) and then run the cell.\n",
    "\n",
    "By default we use the same modified version of the Adam optimization algorithm that Mesh Transformer JAX uses by default for training.\n",
    "\n",
    "The main thing you have to pay attention to is `lr` and `max_grad_norm`; everything else is basically universally OK for training. It is recommended to set `lr` to somewhere in the `1e-5` to `5e-5` range. Higher `lr` results in the trainer having a stronger effect. Values higher than around `7e-5` tend to result in exploding gradients (numerical instability) and should be avoided! You can tell when this happens because the Gradient L2 Norm will start increasing abnormally and eventually \"explode\" to values in the thousands. If the trainer still isn't strong enough, you should train with more epochs instead of risking the numerical instability.\n",
    "\n",
    "`max_grad_norm` controls the maximum allowed rate at which the soft prompt can be trained, so if the trainer tries to change the soft prompt by too much in one step, the changes will be scaled down uniformly. Lower values are more restrictive.\n",
    "\n",
    "* `save_every` (`int` > 0): We'll save an MTJSP file every this many steps so that if you are disconnected from Colab, you can continue from an earlier point in the training.\n",
    "* `warmup` (`float` between 0.0 and 1.0 inclusive): What portion of the beginning of the total training steps should be warmup steps. The learning rate for warmup steps starts at 0.0 and increases linearly to the maximum learning rate.\n",
    "* `lr` (`float` > 0): Aforementioned maximum learning rate.\n",
    "* `end_lr_multiplier` (`float` > 0): After the warmup steps, the remaining training steps have a learning rate controlled by a cosine function that goes from the maximum learning rate to this proportion of the maximum learning rate (i.e. the default is one-tenth of the maximum learning rate).\n",
    "* `weight_decay` (`float` between 0.0 and 1.0 inclusive): The soft prompt you're going to train is actually a two-dimensional array of floating-point numbers. The absolute values of the numbers tend to be pretty small (less than 1). If the absolute value of one of those numbers grows too large, the trainer tends to try to reduce the absolute values of the entire array, resulting in the entire array being filled with zeros and ruining your soft prompt. This setting effectively restricts the absolute value of the numbers in the array (higher weight decay value means the maximum absolute value is lower) to help prevent this scenario. Of course, setting it too high would lower the absolute value of your array, too, just like what would happen if you'd set it too low, so there's usually an optimal value. People have suggested 0.1 as a good all-around weight decay factor.\n",
    "* `max_grad_norm` (`float` > 0): Controls the maximum allowed rate at which the soft prompt can be trained, so if the trainer tries to change the soft prompt by too much in one step, the changes will be scaled down uniformly. Lower values are more restrictive. If the \"Gradient L2 Norm\" is much higher (e.g. at least twice as much) on average than this value, you should probably raise this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "zrD3E45K1cx9"
   },
   "outputs": [],
   "source": [
    "lr = 3e-5  # @param {type:\"number\"}\n",
    "max_grad_norm = 10.0  # @param {type:\"number\"}\n",
    "weight_decay = 0.1  # @param {type:\"number\"}\n",
    "warmup = 0.1  # @param {type:\"number\"}\n",
    "end_lr_multiplier = 0.1  # @param {type:\"number\"}\n",
    "save_every = 50  # @param {type:\"integer\"}\n",
    "stparams = {\n",
    "    param: globals()[param]\n",
    "    for param in (\n",
    "        \"lr\",\n",
    "        \"max_grad_norm\",\n",
    "        \"weight_decay\",\n",
    "        \"warmup\",\n",
    "        \"end_lr_multiplier\",\n",
    "        \"save_every\",\n",
    "    )\n",
    "}\n",
    "\n",
    "save_variable(\"stparams\", stparams)\n",
    "print(\"OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHHdFg5ZBIEu"
   },
   "source": [
    "<br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9LNutONBJrU"
   },
   "source": [
    "# 4. Soft-tune the model\n",
    "\n",
    "## If you reached here and at any point after that restarted your Colab instance by using Runtime > Restart Runtime, you will have to run the step 1 cell again and then run the cells below again. You don't have to re-run any cells in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p369Lt4PYHRE"
   },
   "source": [
    "This can take quite a while depending on how fast your Colab instance is. Note that currently (2021-10-12), all Colab TPU instances will train at the same speed, some just take longer to initialize the model than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5pqnXdpYuS9"
   },
   "outputs": [],
   "source": [
    "if \"ckpt_path\" not in globals():\n",
    "    raise NotebookException(\"You didn't specify the path to your model.\")\n",
    "elif \"params\" not in globals():\n",
    "    raise NotebookException(\n",
    "        \"You have not specified whether you're using a GPT-J-6B or GPT-Neo-2.7B model.\"\n",
    "    )\n",
    "elif starting_step == -1:\n",
    "    raise NotebookException(\n",
    "        \"You did not set an initial soft prompt string to begin soft-tuning with or existing save file to resume soft-tuning with.\"\n",
    "    )\n",
    "if \"dataset_file\" not in globals():\n",
    "    raise NotebookException(\"You have not specified the path to your npy dataset file.\")\n",
    "\n",
    "step = starting_step\n",
    "\n",
    "# Set up the scheduler which determines the learning rate for each step\n",
    "steps = num_sequences // gradient_accumulation_steps\n",
    "warmup_steps = max(1, round(steps * stparams[\"warmup\"]))\n",
    "scheduler = mesh_transformer.util.gpt3_schedule(\n",
    "    warmup_steps,\n",
    "    max(1, steps - warmup_steps),\n",
    "    stparams[\"lr\"],\n",
    "    stparams[\"end_lr_multiplier\"] * stparams[\"lr\"],\n",
    ")\n",
    "\n",
    "# Tell Mesh Transformer to create the network as bfloat16\n",
    "params[\"early_cast\"] = True\n",
    "\n",
    "if step == 0:\n",
    "    print(\"We are starting a brand new soft-tuning session.\\n\")\n",
    "else:\n",
    "    # If we're resuming a soft-tuning session, the soft prompt tensor is\n",
    "    # already in the save file and we just have to decode it.\n",
    "    try:\n",
    "        npz = np.load(save_file, allow_pickle=True)\n",
    "        assert npz[\"step\"] > 0\n",
    "        assert npz[\"tensor\"].ndim == 2 and \"opt_state\" in npz\n",
    "        assert npz[\"tensor\"].shape[0] < seq\n",
    "        assert npz[\"tensor\"].shape[1] == params[\"d_model\"]\n",
    "        assert all(\n",
    "            p in npz\n",
    "            for p in (\n",
    "                \"loss\",\n",
    "                \"last_loss\",\n",
    "                \"grad_norm\",\n",
    "                \"grad_norm_micro\",\n",
    "            )\n",
    "        )\n",
    "        assert soft_in_dim == npz[\"tensor\"].shape[0]\n",
    "        step = np.uint32(npz[\"step\"]).item()\n",
    "    except:\n",
    "        raise NotebookException(\"MTJSP file is corrupted.\")\n",
    "    print(f\"We're resuming a previous soft-tuning session at step {step+1}.\\n\")\n",
    "    soft_embeddings = npz[\"tensor\"]\n",
    "    if soft_embeddings.dtype == \"V2\":\n",
    "        soft_embeddings.dtype = jnp.bfloat16\n",
    "    soft_embeddings = jnp.float32(soft_embeddings)\n",
    "\n",
    "# Load the model\n",
    "if \"spmodule\" not in globals():\n",
    "    print(termcolor.colored(\"Initializing network...\", \"magenta\"), flush=True)\n",
    "    params[\"optimizer\"] = optax.scale(0)\n",
    "    network = EmbeddingCausalTransformer(params)\n",
    "    print(termcolor.colored(\"\\n\\nLoading pretrained model...\", \"magenta\"), flush=True)\n",
    "    network.state, spmodule = read_ckpt_custom(\n",
    "        network.state, ckpt_path, params[\"cores_per_replica\"]\n",
    "    )\n",
    "    network.state = network.move_xmap(\n",
    "        network.state, np.zeros(params[\"cores_per_replica\"])\n",
    "    )\n",
    "    network.state[\"params\"][spmodule][\"w\"] = jnp.float32(\n",
    "        network.state[\"params\"][spmodule][\"w\"]\n",
    "    )\n",
    "\n",
    "# Set up the optimizer, which is the algorithm we use to train the soft prompt\n",
    "params[\"optimizer\"] = network.config[\"optimizer\"] = optax.chain(\n",
    "    optax.scale(1 / gradient_accumulation_steps),\n",
    "    mesh_transformer.util.clip_by_global_norm(float(stparams[\"max_grad_norm\"])),\n",
    "    optax.scale_by_adam(mu_dtype=jnp.float32),\n",
    "    mesh_transformer.util.additive_weight_decay(stparams[\"weight_decay\"]),\n",
    "    optax.scale(-1),\n",
    "    optax.scale_by_schedule(scheduler),\n",
    ")\n",
    "\n",
    "if step == 0:\n",
    "    # If we're starting a soft-tuning session from scratch, we initialize the\n",
    "    # soft prompt tensor by using the model to \"embed\" the tokens from the\n",
    "    # initial soft prompt string, producing a matrix (2D array) that we use as\n",
    "    # the soft prompt tensor.\n",
    "    soft_embeddings = network.get_embedding_matrix(\n",
    "        np.array(initial_softprompt, dtype=np.uint32)\n",
    "    )\n",
    "    soft_embeddings = jnp.float32(soft_embeddings)\n",
    "    # We also have to initialize the optimizer state in that case\n",
    "    network.state[\"opt_state\"] = init_opt_state(network.state[\"params\"][spmodule])\n",
    "else:\n",
    "    # Optimizer state is already saved otherwise\n",
    "    network.state[\"opt_state\"] = mesh_transformer.util.to_f32(tuple(npz[\"opt_state\"]))\n",
    "\n",
    "# Pad the embedding matrix with zeros at the bottom so that its number of\n",
    "# rows is a multiple of 8 (or 4 for GPT-Neo-2.7B)\n",
    "rows = soft_embeddings.shape[0]\n",
    "padding_amount = -(rows % -params[\"cores_per_replica\"])\n",
    "soft_embeddings = jnp.pad(soft_embeddings, ((0, padding_amount), (0, 0)))\n",
    "# Split the matrix row-wise into 8 (or 4) submatrices (so that if the original\n",
    "# matrix had R rows and C columns, then each submatrix has R/8 rows and C\n",
    "# columns) and then concatenate the 8 submatrices together along a new\n",
    "# leading axis into a 3-dimensional array so that it can be sharded by\n",
    "# xmapped functions\n",
    "soft_embeddings = soft_embeddings.reshape(\n",
    "    (params[\"cores_per_replica\"], -1, params[\"d_model\"])\n",
    ")\n",
    "# Put this 3D array into the network so we can train it\n",
    "network.state[\"params\"][spmodule][\"w\"] = soft_embeddings\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def save_mtjsp(\n",
    "    loss,\n",
    "    last_loss,\n",
    "    grad_norm,\n",
    "    grad_norm_micro,\n",
    "):\n",
    "    global starting_step\n",
    "    tensor = network.state[\"params\"][spmodule][\"w\"]\n",
    "    tensor = tensor.reshape((-1, tensor.shape[2]))\n",
    "    tensor = tensor[:soft_in_dim]\n",
    "    with open(save_file, \"wb\") as f:\n",
    "        np.savez_compressed(\n",
    "            f,\n",
    "            tensor=tensor,\n",
    "            opt_state=np.array(network.state[\"opt_state\"], dtype=np.object),\n",
    "            step=np.uint32(step),\n",
    "            loss=np.float32(loss),\n",
    "            last_loss=np.float32(last_loss),\n",
    "            grad_norm=np.float32(grad_norm),\n",
    "            grad_norm_micro=np.float32(grad_norm_micro),\n",
    "        )\n",
    "    starting_step = step\n",
    "    save_variable(\"starting_step\", starting_step)\n",
    "\n",
    "\n",
    "def push_data(\n",
    "    loss,\n",
    "    last_loss,\n",
    "    grad_norm,\n",
    "    grad_norm_micro,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the training plots with the given data.\n",
    "    \"\"\"\n",
    "    IPython.display.display(\n",
    "        IPython.display.Javascript(\n",
    "            f\"push({step}, {loss}, {grad_norm}, {scheduler(step)}); update();\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def train_step(use_tqdm=True):\n",
    "    # Get the next batch from the dataset\n",
    "    data = dataset[\n",
    "        (step - 1) * gradient_accumulation_steps : step * gradient_accumulation_steps\n",
    "    ]\n",
    "    # Concatenate the soft prompt at the beginning\n",
    "    vocab_size = params[\"n_vocab\"] + params.get(\"n_vocab_padding\", 0)\n",
    "    header = np.tile(\n",
    "        np.arange(vocab_size, vocab_size + soft_in_dim, dtype=np.uint32),\n",
    "        (data.shape[0], 1),\n",
    "    )\n",
    "    data = np.concatenate((header, data), axis=1)[:, : seq + 1]\n",
    "\n",
    "    ctx = data[:, :-1]\n",
    "    tgt = data[:, 1:]\n",
    "\n",
    "    grad, loss, last_loss, gnorm = train_initial(\n",
    "        network.state,\n",
    "        ctx[np.newaxis, 0],\n",
    "        tgt[np.newaxis, 0],\n",
    "    )\n",
    "    r = range(1, ctx.shape[0])\n",
    "    if use_tqdm:\n",
    "        r = tqdm(\n",
    "            r,\n",
    "            initial=1,\n",
    "            total=ctx.shape[0],\n",
    "            desc=\"GRADIENT ACCUMULATION\",\n",
    "            leave=False,\n",
    "        )\n",
    "    for i in r:\n",
    "        grad, _loss, _last_loss, _gnorm = train_intermediate(\n",
    "            network.state,\n",
    "            grad,\n",
    "            ctx[np.newaxis, i],\n",
    "            tgt[np.newaxis, i],\n",
    "        )\n",
    "        loss += _loss\n",
    "        last_loss += _last_loss\n",
    "        gnorm += _gnorm\n",
    "    loss /= ctx.shape[0]\n",
    "    last_loss /= ctx.shape[0]\n",
    "    gnorm /= ctx.shape[0]\n",
    "    grad_norm, grad_norm_micro, network.state = train_final(\n",
    "        network.state,\n",
    "        grad,\n",
    "        gnorm,\n",
    "    )\n",
    "    del grad\n",
    "\n",
    "    return (\n",
    "        np.array(loss).mean(),\n",
    "        np.array(last_loss).mean(),\n",
    "        np.array(grad_norm).mean(),\n",
    "        np.array(grad_norm_micro).mean(),\n",
    "    )\n",
    "\n",
    "\n",
    "step += 1\n",
    "\n",
    "\n",
    "# Train\n",
    "first_step = step\n",
    "if step <= steps:\n",
    "    # Load the dataset\n",
    "    dataset = np.load(dataset_file, mmap_mode=\"r\")\n",
    "    assert dataset.ndim >= 2\n",
    "    assert dataset.shape[1] >= 2\n",
    "    assert dataset.shape[0] == num_sequences\n",
    "    print(\n",
    "        termcolor.colored(\n",
    "            \"Compiling trainer, this may take several minutes\\n\", \"magenta\"\n",
    "        ),\n",
    "        flush=True,\n",
    "    )\n",
    "    # Simultaneously compile the trainer and train for one step\n",
    "    loss, last_loss, grad_norm, grad_norm_micro = train_step(use_tqdm=False)\n",
    "    # Show the plots for learning rate, etc.\n",
    "    IPython.display.clear_output()\n",
    "    IPython.display.display(IPython.core.display.HTML(plot_html))\n",
    "    # Update plot\n",
    "    push_data(\n",
    "        loss,\n",
    "        last_loss,\n",
    "        grad_norm,\n",
    "        grad_norm_micro,\n",
    "    )\n",
    "# Create a save file for step 1\n",
    "if step == 1 or step % stparams[\"save_every\"] == 0:\n",
    "    save_mtjsp(\n",
    "        loss,\n",
    "        last_loss,\n",
    "        grad_norm,\n",
    "        grad_norm_micro,\n",
    "    )\n",
    "for i in tqdm(\n",
    "    range(first_step, steps),\n",
    "    initial=first_step,\n",
    "    total=steps,\n",
    "    desc=\"SOFT-TUNING PROGRESS\",\n",
    "):\n",
    "    step += 1\n",
    "    # Train for one step and update the plot\n",
    "    loss, last_loss, grad_norm, grad_norm_micro = train_step(use_tqdm=True)\n",
    "    push_data(\n",
    "        loss,\n",
    "        last_loss,\n",
    "        grad_norm,\n",
    "        grad_norm_micro,\n",
    "    )\n",
    "    # Save whenever step is divisible by save_every\n",
    "    if step % stparams[\"save_every\"] == 0:\n",
    "        save_mtjsp(\n",
    "            loss,\n",
    "            last_loss,\n",
    "            grad_norm,\n",
    "            grad_norm_micro,\n",
    "        )\n",
    "step += 1\n",
    "save_mtjsp(\n",
    "    loss,\n",
    "    last_loss,\n",
    "    grad_norm,\n",
    "    grad_norm_micro,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7WWC-v_Z_fD"
   },
   "source": [
    "Use this cell to login to Google Drive if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EhnFqu_aDED"
   },
   "outputs": [],
   "source": [
    "google.colab.drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQG-57PCS63x"
   },
   "source": [
    "Once you finish soft-tuning, you can use the following cell to convert your MTJSP file to a KoboldAI United-compatible ZIP file:\n",
    "\n",
    "(`supported` should be the name of a model or a comma-separated list of such names that the soft prompt is intended to be used with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "v9KYKTw1S63y"
   },
   "outputs": [],
   "source": [
    "output_file = \"/content/drive/MyDrive/my_softprompt.zip\"  # @param {type:\"string\"}\n",
    "name = \"Untitled\"  # @param {type:\"string\"}\n",
    "author = \"\"  # @param {type:\"string\"}\n",
    "supported = \"Generic 6B\"  # @param {type:\"string\"}\n",
    "description = \"Baby shoes\"  # @param {type:\"string\"}\n",
    "\n",
    "try:\n",
    "    npz = np.load(save_file, allow_pickle=True)\n",
    "    assert npz[\"step\"] > 0\n",
    "    assert npz[\"tensor\"].ndim == 2 and \"opt_state\" in npz\n",
    "    assert npz[\"tensor\"].shape[0] < seq\n",
    "    assert npz[\"tensor\"].shape[1] == params[\"d_model\"]\n",
    "    assert all(\n",
    "        p in npz\n",
    "        for p in (\n",
    "            \"loss\",\n",
    "            \"last_loss\",\n",
    "            \"grad_norm\",\n",
    "            \"grad_norm_micro\",\n",
    "        )\n",
    "    )\n",
    "    _step = np.uint32(npz[\"step\"]).item()\n",
    "except:\n",
    "    raise NotebookException(\"MTJSP file is corrupted.\")\n",
    "\n",
    "tensor = npz[\"tensor\"]\n",
    "if tensor.dtype == \"V2\":\n",
    "    tensor.dtype = jnp.bfloat16\n",
    "\n",
    "meta = {\n",
    "    k: globals()[k]\n",
    "    for k in (\n",
    "        \"name\",\n",
    "        \"author\",\n",
    "        \"supported\",\n",
    "        \"description\",\n",
    "    )\n",
    "}\n",
    "if len(meta[\"author\"].strip()) == 0:\n",
    "    meta.pop(\"author\")\n",
    "meta[\"supported\"] = list(map(lambda m: m.strip(), supported.split(\",\")))\n",
    "\n",
    "with zipfile.ZipFile(output_file, \"w\", compression=zipfile.ZIP_LZMA) as z:\n",
    "    with z.open(\"tensor.npy\", \"w\") as f:\n",
    "        np.save(f, tensor, allow_pickle=False)\n",
    "with zipfile.ZipFile(output_file, \"a\", compression=zipfile.ZIP_STORED) as z:\n",
    "    with z.open(\"meta.json\", \"w\") as f:\n",
    "        f.write(json.dumps(meta, indent=2).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaicnpyGZS0J"
   },
   "source": [
    "Or this one, to convert your MTJSP file to an mkultra-compatible JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NbHANr-bZSIs"
   },
   "outputs": [],
   "source": [
    "output_file = \"/content/drive/MyDrive/my_softprompt.json\"  # @param {type:\"string\"}\n",
    "soft_prompt_name = \"Untitled\"  # @param {type:\"string\"}\n",
    "soft_prompt_description = \"Baby shoes\"  # @param {type:\"string\"}\n",
    "\n",
    "try:\n",
    "    npz = np.load(save_file, allow_pickle=True)\n",
    "    assert npz[\"step\"] > 0\n",
    "    assert npz[\"tensor\"].ndim == 2 and \"opt_state\" in npz\n",
    "    assert npz[\"tensor\"].shape[0] < seq\n",
    "    assert npz[\"tensor\"].shape[1] == params[\"d_model\"]\n",
    "    assert all(\n",
    "        p in npz\n",
    "        for p in (\n",
    "            \"loss\",\n",
    "            \"last_loss\",\n",
    "            \"grad_norm\",\n",
    "            \"grad_norm_micro\",\n",
    "        )\n",
    "    )\n",
    "    _step = np.uint32(npz[\"step\"]).item()\n",
    "except:\n",
    "    raise NotebookException(\"MTJSP file is corrupted.\")\n",
    "\n",
    "tensor = npz[\"tensor\"]\n",
    "if tensor.dtype == \"V2\":\n",
    "    tensor.dtype = jnp.bfloat16\n",
    "    tensor = torch.tensor(tensor).to(torch.float32)\n",
    "else:\n",
    "    tensor = torch.tensor(tensor)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"metadata\": {\n",
    "                \"step\": _step,\n",
    "                \"loss\": float(npz[\"loss\"].item()),\n",
    "                \"uuid\": str(uuid.uuid4()),\n",
    "                \"name\": soft_prompt_name,\n",
    "                \"description\": soft_prompt_description,\n",
    "                \"epoch\": datetime.datetime.now().timestamp(),\n",
    "            },\n",
    "            \"tensor\": base64.b64encode(\n",
    "                pickle.dumps(\n",
    "                    tensor,\n",
    "                    protocol=4,\n",
    "                ),\n",
    "            ).decode(\"ascii\"),\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "\n",
    "print(\"OK.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mtj-softtuner.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
